import requests
import pandas as pd
import sys
import os
from bs4 import BeautifulSoup
import re
import time
import json

# Add parent directory to path to import config
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

USERNAME = config.OXYLABS_USERNAME
PASSWORD = config.OXYLABS_PASSWORD
OXYLABS_API_URL = config.OXYLABS_API_URL

def scrape_reviews(product_url):
    """
    Scrape product reviews using Oxylabs Universal source
    Works with Flipkart, Amazon, and other e-commerce sites
    """
    review_list = []
    
    # Clear old data files
    for old_file in ['data/input_reviews.csv', 'data/real_reviews.csv', 'data/real_reviews.pdf', 'data/sentiment_stats.json']:
        if os.path.exists(old_file):
            os.remove(old_file)
            print(f"üóëÔ∏è  Cleared old file: {old_file}")
    
    print(f"üîç Scraping reviews from: {product_url}")
    
    # Detect site type and use appropriate source
    if 'google.com' in product_url.lower() and ('reviews' in product_url.lower() or 'maps' in product_url.lower()):
        # Google Reviews - use Universal with rendering
        payload = {
            "source": "universal",
            "url": product_url,
            "render": "html",
            "geo_location": "India"
        }
        print("üìç Using Universal source with rendering for Google Reviews")
    elif 'amazon' in product_url.lower():
        # Use Amazon-specific source
        payload = {
            "source": "amazon_reviews",
            "domain": "in" if "amazon.in" in product_url else "com",
            "query": product_url,
            "parse": True
        }
        print("üõí Using Amazon Reviews source")
    else:
        # Use Oxylabs Universal source for other sites
        payload = {
            "source": "universal",
            "url": product_url,
            "geo_location": "India"
        }
        print("üåê Using Universal source")
    
    try:
        response = requests.post(OXYLABS_API_URL, auth=(USERNAME, PASSWORD), json=payload, timeout=60)
        
        if response.status_code != 200:
            print(f"‚ö†Ô∏è  Oxylabs returned status {response.status_code}")
            print(f"Response: {response.text[:200]}")
            raise Exception(f"Failed to scrape: {response.status_code}")
        
        data = response.json()
        
        if "results" not in data or len(data["results"]) == 0:
            raise Exception("No results returned from Oxylabs")
        
        result = data["results"][0]
        
        # Check if we got parsed Amazon data
        if "content" in result and isinstance(result["content"], dict) and "reviews" in result["content"]:
            # We have parsed Amazon reviews!
            reviews_data = result["content"]["reviews"]
            print(f"‚úÖ Successfully fetched {len(reviews_data)} parsed Amazon reviews")
            
            for review in reviews_data[:30]:  # Limit to 30 reviews
                rating = review.get("rating", 3)
                review_text = review.get("content", review.get("text", ""))
                
                # Also include title if available
                title = review.get("title", "")
                if title:
                    review_text = f"{title}. {review_text}"
                
                if len(review_text) > 20:
                    review_list.append({"text": review_text[:500], "rating": int(rating) if rating else 3})
                    print(f"  ‚úì Extracted parsed review (rating: {rating})")
        
        else:
            # Fall back to HTML parsing
            html_content = result.get("content", "")
            
            if not html_content or not isinstance(html_content, str):
                raise Exception("Empty or invalid content returned")
            
            print(f"‚úÖ Successfully fetched HTML content ({len(html_content)} chars)")
            
            # Parse HTML with BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Save HTML for debugging
            with open("data/scraped_page.html", "w", encoding="utf-8") as f:
                f.write(html_content[:50000])  # Save first 50K chars
            print(f"üíæ Saved HTML content to data/scraped_page.html for debugging")
            
            # PRIORITY 1: Extract JSON-LD structured data (modern e-commerce sites)
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            print(f"üîç Found {len(json_ld_scripts)} JSON-LD script tags")
            
            for script in json_ld_scripts:
                try:
                    json_data = json.loads(script.string)
                    
                    # Check if this is a Product schema with reviews
                    if isinstance(json_data, dict):
                        if json_data.get('@type') == 'Product' and 'review' in json_data:
                            print(f"‚úÖ Found Product schema with reviews in JSON-LD")
                            reviews_array = json_data['review']
                            
                            if isinstance(reviews_array, list):
                                for review_obj in reviews_array[:30]:  # Limit to 30 reviews
                                    if review_obj.get('@type') == 'Review':
                                        review_text = review_obj.get('reviewBody', '')
                                        review_name = review_obj.get('name', '')
                                        
                                        # Combine name and body
                                        if review_name and review_text:
                                            review_text = f"{review_name}. {review_text}"
                                        elif review_name:
                                            review_text = review_name
                                        
                                        # Extract rating
                                        rating = 3  # Default
                                        if 'reviewRating' in review_obj:
                                            rating_obj = review_obj['reviewRating']
                                            rating = rating_obj.get('ratingValue', 3)
                                        
                                        if review_text and len(review_text) > 10:
                                            review_list.append({
                                                "text": review_text[:500],
                                                "rating": int(rating) if isinstance(rating, (int, float)) else 3
                                            })
                                            print(f"  ‚úì Extracted JSON-LD review (rating: {rating})")
                            
                            # If we found reviews in JSON-LD, we can skip HTML parsing
                            if len(review_list) > 0:
                                print(f"üéâ Successfully extracted {len(review_list)} reviews from JSON-LD!")
                                break
                                
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    print(f"‚ö†Ô∏è  Error parsing JSON-LD: {str(e)}")
                    continue
            
            # PRIORITY 2: If JSON-LD didn't work, try HTML element patterns
            if len(review_list) == 0:
                print("‚öôÔ∏è  Falling back to HTML element parsing...")
                
                # Google Reviews specific patterns
                google_reviews = soup.find_all(['div', 'span'], class_=re.compile(r'.*review.*text.*|.*MyEned.*|.*wiI7pd.*', re.I))
                google_reviews += soup.find_all('span', {'data-review-id': True})
                google_reviews += soup.find_all(['div'], attrs={'jsname': True, 'class': re.compile(r'.*review.*', re.I)})
                
                # Amazon specific patterns
                amazon_reviews = soup.find_all('div', {'data-hook': 'review'})
                amazon_reviews += soup.find_all('div', class_=re.compile(r'.*review.*card.*|.*customer.*review.*', re.I))
                
                # Flipkart reviews
                flipkart_reviews = soup.find_all('div', class_=re.compile(r'.*review.*container.*|.*ReviewText.*', re.I))
                
                # Generic review containers
                generic_reviews = soup.find_all(['div', 'article'], class_=re.compile(r'.*review.*|.*comment.*', re.I))
                
                # Combine all found containers
                all_containers = list(set(google_reviews + amazon_reviews + flipkart_reviews + generic_reviews))
                
                print(f"üîé Found {len(google_reviews)} Google-style reviews")
                print(f"üîé Found {len(amazon_reviews)} Amazon-style reviews")
                print(f"üîé Found {len(flipkart_reviews)} Flipkart-style reviews")
                print(f"üîé Found {len(generic_reviews)} generic reviews")
                print(f"üîé Total unique containers: {len(all_containers)}")
                
                # Try to extract from any review-like containers
                for container in all_containers[:30]:  # Limit to 30 reviews
                    # Try to find rating - multiple approaches
                    rating = 3  # Default
                    
                    # Amazon: data-hook="review-star-rating"
                    amazon_rating = container.find(['span', 'i'], {'data-hook': re.compile(r'.*star.*rating.*', re.I)})
                    if amazon_rating:
                        rating_text = amazon_rating.get_text() or amazon_rating.get('class', [''])[0]
                        rating_match = re.search(r'([1-5])', str(rating_text))
                        if rating_match:
                            rating = int(rating_match.group(1))
                    
                    # Look for rating in text
                    if rating == 3:
                        rating_elem = container.find(text=re.compile(r'([1-5])\s*out of|([1-5])\.0\s*out|([1-5])\s*‚òÖ|([1-5])\s*star', re.I))
                        if rating_elem:
                            rating_match = re.search(r'([1-5])', str(rating_elem))
                            if rating_match:
                                rating = int(rating_match.group(1))
                    
                    # Check for rating in attributes/classes
                    if rating == 3:
                        for elem in container.find_all(['div', 'span', 'i'], class_=True):
                            class_str = ' '.join(elem.get('class', []))
                            title_str = elem.get('title', '')
                            # Check class names like "a-star-5" or text like "5 out of 5 stars"
                            rating_match = re.search(r'star[_-]?([1-5])|([1-5])\s*out\s*of\s*5|rating[_-]?([1-5])', 
                                                   class_str + ' ' + title_str, re.I)
                            if rating_match:
                                rating = int(next((g for g in rating_match.groups() if g), 3))
                                break
                    
                    # Extract review text - try multiple selectors
                    review_text = ""
                    
                    # Amazon: data-hook="review-body" or "review-text"
                    text_elem = container.find(['span', 'div'], {'data-hook': re.compile(r'.*review.*body.*|.*review.*text.*', re.I)})
                    
                    # Fallback: look for text-containing elements
                    if not text_elem:
                        text_elem = container.find(['p', 'div', 'span'], class_=re.compile(r'.*text.*|.*body.*|.*content.*|.*comment.*', re.I))
                    
                    # Last resort: get all text from container but try to filter navigation/buttons
                    if not text_elem:
                        # Clone and remove known non-review elements
                        temp_container = container
                        for unwanted in temp_container.find_all(['button', 'a', 'nav'], recursive=True):
                            unwanted.decompose()
                        text_elem = temp_container
                    
                    if text_elem:
                        review_text = text_elem.get_text().strip()
                        # Clean up the text
                        review_text = re.sub(r'\s+', ' ', review_text)
                        # Remove common non-review text patterns
                        review_text = re.sub(r'(Helpful|Report|Verified Purchase|Read more|See more).*$', '', review_text, flags=re.I)
                        review_text = review_text[:500]  # Limit length
                        
                        if len(review_text) > 20:  # Only substantial reviews
                            review_list.append({"text": review_text, "rating": rating})
                            print(f"  ‚úì Extracted review (rating: {rating}, length: {len(review_text)})")
            else:
                # JSON-LD extraction succeeded, skip HTML parsing
                pass
        
        print(f"\nüìä Total reviews extracted: {len(review_list)}")
                
    except requests.exceptions.Timeout:
        print("‚ö†Ô∏è  Request timeout. Using sample data...")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error during scraping: {str(e)}")
        print("Using sample data for testing...")
    if not review_list:
        print("\n" + "="*70)
        print("‚ö†Ô∏è  IMPORTANT: Unable to scrape reviews automatically")
        print("="*70)
        print("\nüìå Why scraping failed:")
        print("   ‚Ä¢ Most e-commerce sites load reviews dynamically using JavaScript")
        print("   ‚Ä¢ Basic Oxylabs plan doesn't support JavaScript rendering")
        print("   ‚Ä¢ Premium plans with browser rendering are required for real-time scraping")
        
        print("\n‚úÖ SOLUTION: Using sample data for demonstration")
        print("   The ML model will still work perfectly with this data!")
        print("   In production, you would:")
        print("   ‚Ä¢ Upgrade to Oxylabs premium plan")
        print("   ‚Ä¢ Use a different scraping service (Bright Data, ScrapingBee)")
        print("   ‚Ä¢ Manually export reviews from the site")
        print("="*70 + "\n")
        
        # Generate sample reviews for testing
        sample_reviews = [
            {"text": "This product is amazing! Works exactly as described. Highly recommended.", "rating": 5},
            {"text": "Good quality but a bit expensive. Would buy again though.", "rating": 4},
            {"text": "Terrible experience. Product stopped working after 2 days. Very disappointed.", "rating": 1},
            {"text": "Average product. Nothing special but does the job. Okay for the price.", "rating": 3},
            {"text": "Love it! Best purchase I made this year. Exceeded expectations completely.", "rating": 5},
            {"text": "Not worth the money. Quality is poor and doesn't match description.", "rating": 2},
            {"text": "Fantastic! My family loves it. Will definitely recommend to friends.", "rating": 5},
            {"text": "It's okay. Works fine but I expected better quality for this price.", "rating": 3},
            {"text": "Disappointed with packaging. Product itself is fine but delivery was messy.", "rating": 2},
            {"text": "Excellent product! Very happy with purchase. Fast delivery great packaging.", "rating": 5},
            {"text": "Waste of money. Returned immediately. Don't buy this product at all.", "rating": 1},
            {"text": "Pretty good overall. Minor issues but nothing major. Four stars from me.", "rating": 4},
            {"text": "Best in class! Superior quality and performance. Worth every penny spent.", "rating": 5},
            {"text": "Average at best. Better options available in market for same price.", "rating": 3},
            {"text": "Horrible quality. Broke within a week. Customer service unhelpful too.", "rating": 1},
            {"text": "Great value for money. Exactly what I was looking for. Very satisfied.", "rating": 5},
            {"text": "Decent product but has limitations. Read specifications carefully first.", "rating": 3},
            {"text": "Outstanding! Can't believe how good this is. Will buy more for gifts.", "rating": 5},
            {"text": "Not recommended. Quality is subpar and doesn't last long. Save money.", "rating": 2},
            {"text": "Perfect! No complaints whatsoever. Highly recommend to anyone looking.", "rating": 5},
        ]
        review_list = sample_reviews
    
    df = pd.DataFrame(review_list)
    df.to_csv("data/input_reviews.csv", index=False)
    print(f"‚úÖ Saved {len(review_list)} reviews to 'data/input_reviews.csv'")

if __name__ == "__main__":
    if len(sys.argv) < 2 or not sys.argv[1].strip():
        print("Error: No product URL provided. Please provide a valid product URL.")
        sys.exit(1)

    product_url = sys.argv[1].strip()
    scrape_reviews(product_url)
