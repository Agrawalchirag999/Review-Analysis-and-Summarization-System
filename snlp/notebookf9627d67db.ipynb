{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9c0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages with specific versions...\n",
      "‚úÖ matplotlib>=3.5.0 installed successfully\n",
      "‚úÖ seaborn>=0.11.0 installed successfully\n",
      "‚úÖ scikit-learn>=1.0.0 installed successfully\n",
      "‚úÖ nltk>=3.6.0 installed successfully\n",
      "‚úÖ torch>=1.9.0 installed successfully\n",
      "‚úÖ transformers>=4.0.0 installed successfully\n",
      "‚úÖ tqdm>=4.60.0 installed successfully\n",
      "‚úÖ pandas>=1.3.0 installed successfully\n",
      "‚úÖ numpy>=1.21.0 installed successfully\n",
      "Package installation complete!\n",
      "‚ùå Import error: No module named 'matplotlib.backends.registry'\n"
     ]
    }
   ],
   "source": [
    "# Install required packages with specific versions\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package])\n",
    "\n",
    "packages = [\n",
    "    \"matplotlib>=3.5.0\",\n",
    "    \"seaborn>=0.11.0\", \n",
    "    \"scikit-learn>=1.0.0\",\n",
    "    \"nltk>=3.6.0\",\n",
    "    \"torch>=1.9.0\",\n",
    "    \"transformers>=4.0.0\",\n",
    "    \"tqdm>=4.60.0\",\n",
    "    \"pandas>=1.3.0\",\n",
    "    \"numpy>=1.21.0\"\n",
    "]\n",
    "\n",
    "print(\"Installing required packages with specific versions...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        install(package)\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error installing {package}: {e}\")\n",
    "\n",
    "print(\"Package installation complete!\")\n",
    "\n",
    "# Try importing to check if everything works\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import nltk\n",
    "    import torch\n",
    "    from transformers import BertTokenizer\n",
    "    print(\"‚úÖ All packages imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaca01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (UnicodeEncodeError('utf-8', '\"\"\"\\nAmazon Fake Review Detector - Traditional ML Pipeline\\n=====================================================\\nThis notebook implements a fake review detection system using traditional ML techniques.\\n\"\"\"\\n\\n# ============================================================================\\n# 1. IMPORT LIBRARIES\\n# ============================================================================\\n\\nprint(\"Starting Amazon Fake Review Detector...\")\\nprint(\"=\" * 70)\\n\\nimport pandas as pd\\nimport numpy as np\\nimport re\\nimport warnings\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n# Core ML libraries\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\nfrom sklearn.metrics import (classification_report, confusion_matrix, \\n                             accuracy_score, precision_recall_fscore_support)\\n\\n# Try to import NLTK\\ntry:\\n    import nltk\\n    from nltk.corpus import stopwords\\n    from nltk.tokenize import word_tokenize\\n    from nltk.stem import WordNetLemmatizer\\n    \\n    # Download NLTK data\\n    nltk.download(\\'punkt\\', quiet=True)\\n    nltk.download(\\'stopwords\\', quiet=True)\\n    nltk.download(\\'wordnet\\', quiet=True)\\n    NLTK_AVAILABLE = True\\n    print(\"‚úÖ NLTK libraries imported and data downloaded!\")\\nexcept ImportError:\\n    print(\"‚ö†Ô∏è NLTK not available, using basic text processing\")\\n    NLTK_AVAILABLE = False\\n\\nprint(\"‚úÖ Core libraries imported successfully!\\\\n\")\\n\\n# ============================================================================\\n# 2. LOAD AND EXPLORE DATA\\n# ============================================================================\\n\\nprint(\"=\" * 70)\\nprint(\"STEP 1: DATA EXPLORATION\")\\nprint(\"=\" * 70)\\n\\n# Load local CSV files\\nprint(\"üìÅ Loading local CSV files...\")\\ntry:\\n    df = pd.read_csv(\\'train.csv\\')\\n    print(f\"‚úÖ Training data loaded: {df.shape}\")\\n    \\n    # Display basic information\\n    print(f\"\\\\nDataset Info:\")\\n    print(f\"Total training samples: {len(df)}\")\\n    \\n    if \\'is_fake\\' in df.columns:\\n        print(f\"Class Distribution:\\\\n{df[\\'is_fake\\'].value_counts()}\")\\n        print(f\"Fake percentage: {df[\\'is_fake\\'].mean() * 100:.2f}%\")\\n    \\n    # Display sample reviews\\n    print(f\"\\\\nSample Reviews:\")\\n    for idx in range(min(3, len(df))):\\n        label = \\'FAKE\\' if df.iloc[idx][\\'is_fake\\'] == 1 else \\'REAL\\' if \\'is_fake\\' in df.columns else \\'Unknown\\'\\n        print(f\"{idx+1}. Label: {label}\")\\n        if \\'rating\\' in df.columns:\\n            print(f\"   Rating: {df.iloc[idx][\\'rating\\']}\")\\n        print(f\"   Text: {df.iloc[idx][\\'text\\'][:100]}...\")\\n        print()\\n        \\nexcept FileNotFoundError:\\n    print(\"‚ùå train.csv not found. Creating sample data for demonstration...\")\\n    # Create sample data\\n    sample_data = {\\n        \\'text\\': [\\n            \"This product is AMAZING!!!! Best purchase ever!!!!! So happy with it! Perfect perfect perfect!\",\\n            \"The product works as described. Good quality for the price. Delivery was on time.\",\\n            \"Terrible product! Complete waste of money! Don\\'t buy this garbage! Worst ever!\",\\n            \"Decent product, delivered on time. Average quality, nothing special.\",\\n            \"BEST PRODUCT EVER!!! LOVE IT SO MUCH!!! BUY NOW!!! Amazing amazing amazing!\",\\n            \"Good product, meets expectations. Fair price for what you get.\",\\n            \"Horrible quality! Broke immediately! Total scam! Don\\'t waste your money!\",\\n            \"Nice product, well made. Would recommend to others. Good value.\",\\n            \"FANTASTIC!!! INCREDIBLE!!! MUST BUY!!! Best thing ever made!!!\",\\n            \"Average product. Does what it\\'s supposed to do. Nothing more, nothing less.\"\\n        ],\\n        \\'rating\\': [5, 4, 1, 3, 5, 4, 1, 4, 5, 3],\\n        \\'is_fake\\': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\\n    }\\n    df = pd.DataFrame(sample_data)\\n    print(f\"‚úÖ Sample data created with {len(df)} reviews\")\\n    print(f\"Class Distribution:\\\\n{df[\\'is_fake\\'].value_counts()}\")\\n\\n# ============================================================================\\n# 3. TEXT PREPROCESSING\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"STEP 2: TEXT PREPROCESSING\")\\nprint(\"=\" * 70)\\n\\nclass TextPreprocessor:\\n    def __init__(self):\\n        if NLTK_AVAILABLE:\\n            try:\\n                self.lemmatizer = WordNetLemmatizer()\\n                self.stop_words = set(stopwords.words(\\'english\\'))\\n                print(\"‚úÖ NLTK preprocessing components loaded\")\\n            except:\\n                self.lemmatizer = None\\n                self.stop_words = set()\\n                print(\"‚ö†Ô∏è Using basic preprocessing (NLTK components failed)\")\\n        else:\\n            self.lemmatizer = None\\n            # Basic stop words\\n            self.stop_words = {\\'the\\', \\'a\\', \\'an\\', \\'and\\', \\'or\\', \\'but\\', \\'in\\', \\'on\\', \\'at\\', \\'to\\', \\'for\\', \\'of\\', \\'with\\', \\'by\\', \\'is\\', \\'are\\', \\'was\\', \\'were\\', \\'be\\', \\'been\\', \\'being\\', \\'have\\', \\'has\\', \\'had\\', \\'do\\', \\'does\\', \\'did\\', \\'will\\', \\'would\\', \\'could\\', \\'should\\', \\'may\\', \\'might\\', \\'must\\', \\'can\\', \\'this\\', \\'that\\', \\'these\\', \\'those\\', \\'i\\', \\'you\\', \\'he\\', \\'she\\', \\'it\\', \\'we\\', \\'they\\', \\'me\\', \\'him\\', \\'her\\', \\'us\\', \\'them\\'}\\n            print(\"‚úÖ Basic preprocessing components loaded\")\\n    \\n    def clean_text(self, text):\\n        \"\"\"Clean and preprocess text\"\"\"\\n        if not isinstance(text, str):\\n            return \"\"\\n        \\n        # Convert to lowercase\\n        text = text.lower()\\n        \\n        # Remove URLs\\n        text = re.sub(r\\'http\\\\S+|www\\\\S+|https\\\\S+\\', \\'\\', text, flags=re.MULTILINE)\\n        \\n        # Remove HTML tags\\n        text = re.sub(r\\'<.*?>\\', \\'\\', text)\\n        \\n        # Remove special characters but keep punctuation\\n        text = re.sub(r\\'[^a-zA-Z\\\\s!?.,]\\', \\'\\', text)\\n        \\n        # Remove extra whitespace\\n        text = re.sub(r\\'\\\\s+\\', \\' \\', text).strip()\\n        \\n        return text\\n    \\n    def tokenize_and_lemmatize(self, text):\\n        \"\"\"Tokenize and lemmatize text\"\"\"\\n        if self.lemmatizer and NLTK_AVAILABLE:\\n            try:\\n                tokens = word_tokenize(text)\\n                lemmatized = [self.lemmatizer.lemmatize(token) for token in tokens]\\n                return \\' \\'.join(lemmatized)\\n            except:\\n                pass\\n        \\n        # Basic tokenization\\n        return text\\n    \\n    def remove_stopwords(self, text):\\n        \"\"\"Remove stopwords\"\"\"\\n        tokens = text.split()\\n        filtered = [word for word in tokens if word.lower() not in self.stop_words]\\n        return \\' \\'.join(filtered)\\n    \\n    def preprocess(self, text, remove_stops=True):\\n        \"\"\"Complete preprocessing pipeline\"\"\"\\n        text = self.clean_text(text)\\n        text = self.tokenize_and_lemmatize(text)\\n        if remove_stops:\\n            text = self.remove_stopwords(text)\\n        return text\\n\\n# Apply preprocessing\\npreprocessor = TextPreprocessor()\\nprint(\"\\\\nüìù Preprocessing text data...\")\\ndf[\\'cleaned_text\\'] = df[\\'text\\'].apply(lambda x: preprocessor.preprocess(x, remove_stops=True))\\n\\nprint(\"‚úÖ Preprocessing complete!\")\\nprint(\"\\\\nExample of preprocessed text:\")\\nprint(f\"Original:  {df.iloc[0][\\'text\\'][:80]}...\")\\nprint(f\"Cleaned:   {df.iloc[0][\\'cleaned_text\\'][:80]}...\")\\n\\n# ============================================================================\\n# 4. FEATURE ENGINEERING\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"STEP 3: FEATURE ENGINEERING\")\\nprint(\"=\" * 70)\\n\\ndef extract_text_features(df):\\n    \"\"\"Extract linguistic features from text\"\"\"\\n    features = pd.DataFrame()\\n    \\n    # Basic text features\\n    features[\\'word_count\\'] = df[\\'text\\'].apply(lambda x: len(str(x).split()))\\n    features[\\'char_count\\'] = df[\\'text\\'].apply(lambda x: len(str(x)))\\n    features[\\'avg_word_length\\'] = df[\\'text\\'].apply(lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0)\\n    \\n    # Sentence features\\n    features[\\'sentence_count\\'] = df[\\'text\\'].apply(lambda x: len(re.split(r\\'[.!?]+\\', str(x))))\\n    features[\\'avg_sentence_length\\'] = features[\\'word_count\\'] / features[\\'sentence_count\\'].replace(0, 1)\\n    \\n    # Punctuation features\\n    features[\\'exclamation_count\\'] = df[\\'text\\'].apply(lambda x: str(x).count(\\'!\\'))\\n    features[\\'question_count\\'] = df[\\'text\\'].apply(lambda x: str(x).count(\\'?\\'))\\n    features[\\'exclamation_ratio\\'] = features[\\'exclamation_count\\'] / (features[\\'word_count\\'] + 1)\\n    \\n    # Capital letters\\n    features[\\'capital_ratio\\'] = df[\\'text\\'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / len(str(x)) if len(str(x)) > 0 else 0)\\n    \\n    # Lexical diversity\\n    features[\\'unique_word_ratio\\'] = df[\\'text\\'].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if len(str(x).split()) > 0 else 0)\\n    \\n    # Extreme words (common in fake reviews)\\n    extreme_words = [\\'amazing\\', \\'awful\\', \\'terrible\\', \\'perfect\\', \\'worst\\', \\'best\\', \\'horrible\\', \\'excellent\\', \\'fantastic\\', \\'incredible\\']\\n    features[\\'extreme_word_count\\'] = df[\\'text\\'].apply(lambda x: sum(str(x).lower().count(word) for word in extreme_words))\\n    features[\\'extreme_word_ratio\\'] = features[\\'extreme_word_count\\'] / (features[\\'word_count\\'] + 1)\\n    \\n    # Rating features (if available)\\n    if \\'rating\\' in df.columns:\\n        features[\\'rating\\'] = df[\\'rating\\']\\n        features[\\'is_extreme_rating\\'] = df[\\'rating\\'].apply(lambda x: 1 if x in [1, 2, 5] else 0)\\n    else:\\n        features[\\'rating\\'] = 3  # neutral default\\n        features[\\'is_extreme_rating\\'] = 0\\n    \\n    return features\\n\\nprint(\"üîß Extracting linguistic features...\")\\ntext_features = extract_text_features(df)\\nprint(f\"‚úÖ Extracted {text_features.shape[1]} features\")\\nprint(\"\\\\nFeature Summary:\")\\nprint(text_features.describe())\\n\\n# ============================================================================\\n# 5. PREPARE DATA FOR MODELING\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"STEP 4: PREPARE DATA FOR MODELING\")\\nprint(\"=\" * 70)\\n\\n# Prepare data\\nX_text = df[\\'cleaned_text\\']\\nX_features = text_features\\ny = df[\\'is_fake\\']\\n\\n# Train-test split\\nX_text_train, X_text_test, y_train, y_test = train_test_split(\\n    X_text, y, test_size=0.2, random_state=42, stratify=y\\n)\\n\\nX_features_train, X_features_test = train_test_split(\\n    X_features, test_size=0.2, random_state=42, stratify=y\\n)\\n\\nprint(f\"‚úÖ Data split complete!\")\\nprint(f\"Training samples: {len(X_text_train)}\")\\nprint(f\"Testing samples: {len(X_text_test)}\")\\nprint(f\"Training class balance: {y_train.value_counts().to_dict()}\")\\n\\n# ============================================================================\\n# 6. TRADITIONAL ML MODELS\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"STEP 5: TRADITIONAL ML MODEL TRAINING\")\\nprint(\"=\" * 70)\\n\\nmodels = {}\\nresults = {}\\n\\n# TF-IDF Vectorizer\\ntfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), min_df=1, max_df=0.95)\\nX_train_tfidf = tfidf.fit_transform(X_text_train)\\nX_test_tfidf = tfidf.transform(X_text_test)\\n\\nprint(f\"TF-IDF feature matrix: {X_train_tfidf.shape}\")\\n\\n# Model 1: Logistic Regression\\nprint(\"\\\\nü§ñ Training Model 1: Logistic Regression...\")\\nlr_model = LogisticRegression(max_iter=1000, random_state=42)\\nlr_model.fit(X_train_tfidf, y_train)\\nlr_pred = lr_model.predict(X_test_tfidf)\\nlr_acc = accuracy_score(y_test, lr_pred)\\n\\nmodels[\\'Logistic Regression\\'] = (tfidf, lr_model)\\nresults[\\'Logistic Regression\\'] = {\\'accuracy\\': lr_acc, \\'predictions\\': lr_pred}\\nprint(f\"‚úÖ Logistic Regression Accuracy: {lr_acc:.4f}\")\\n\\n# Model 2: Naive Bayes\\nprint(\"\\\\nü§ñ Training Model 2: Naive Bayes...\")\\nnb_model = MultinomialNB()\\nnb_model.fit(X_train_tfidf, y_train)\\nnb_pred = nb_model.predict(X_test_tfidf)\\nnb_acc = accuracy_score(y_test, nb_pred)\\n\\nmodels[\\'Naive Bayes\\'] = (tfidf, nb_model)\\nresults[\\'Naive Bayes\\'] = {\\'accuracy\\': nb_acc, \\'predictions\\': nb_pred}\\nprint(f\"‚úÖ Naive Bayes Accuracy: {nb_acc:.4f}\")\\n\\n# Model 3: Random Forest\\nprint(\"\\\\nü§ñ Training Model 3: Random Forest...\")\\nrf_model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10)\\nrf_model.fit(X_train_tfidf, y_train)\\nrf_pred = rf_model.predict(X_test_tfidf)\\nrf_acc = accuracy_score(y_test, rf_pred)\\n\\nmodels[\\'Random Forest\\'] = (tfidf, rf_model)\\nresults[\\'Random Forest\\'] = {\\'accuracy\\': rf_acc, \\'predictions\\': rf_pred}\\nprint(f\"‚úÖ Random Forest Accuracy: {rf_acc:.4f}\")\\n\\n# Model 4: Gradient Boosting\\nprint(\"\\\\nü§ñ Training Model 4: Gradient Boosting...\")\\ngb_model = GradientBoostingClassifier(n_estimators=50, random_state=42, max_depth=5)\\ngb_model.fit(X_train_tfidf, y_train)\\ngb_pred = gb_model.predict(X_test_tfidf)\\ngb_acc = accuracy_score(y_test, gb_pred)\\n\\nmodels[\\'Gradient Boosting\\'] = (tfidf, gb_model)\\nresults[\\'Gradient Boosting\\'] = {\\'accuracy\\': gb_acc, \\'predictions\\': gb_pred}\\nprint(f\"‚úÖ Gradient Boosting Accuracy: {gb_acc:.4f}\")\\n\\n# ============================================================================\\n# 7. MODEL EVALUATION\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"STEP 6: MODEL EVALUATION\")\\nprint(\"=\" * 70)\\n\\n# Model comparison\\ncomparison_data = []\\nfor model_name in results.keys():\\n    preds = results[model_name][\\'predictions\\']\\n    acc = results[model_name][\\'accuracy\\']\\n    precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average=\\'binary\\', zero_division=0)\\n    \\n    comparison_data.append({\\n        \\'Model\\': model_name,\\n        \\'Accuracy\\': acc,\\n        \\'Precision\\': precision,\\n        \\'Recall\\': recall,\\n        \\'F1-Score\\': f1\\n    })\\n\\ncomparison_df = pd.DataFrame(comparison_data).sort_values(\\'Accuracy\\', ascending=False)\\n\\nprint(\"üìä Model Comparison:\")\\nprint(\"-\" * 80)\\nprint(f\"{\\'Model\\':<20} {\\'Accuracy\\':<10} {\\'Precision\\':<12} {\\'Recall\\':<10} {\\'F1-Score\\':<10}\")\\nprint(\"-\" * 80)\\nfor _, row in comparison_df.iterrows():\\n    print(f\"{row[\\'Model\\']:<20} {row[\\'Accuracy\\']:<10.4f} {row[\\'Precision\\']:<12.4f} {row[\\'Recall\\']:<10.4f} {row[\\'F1-Score\\']:<10.4f}\")\\n\\n# Best model analysis\\nbest_model_name = comparison_df.iloc[0][\\'Model\\']\\nbest_predictions = results[best_model_name][\\'predictions\\']\\n\\nprint(f\"\\\\nüèÜ Best Model: {best_model_name}\")\\nprint(f\"Best Accuracy: {comparison_df.iloc[0][\\'Accuracy\\']:.4f}\")\\n\\nprint(f\"\\\\n\\udcc8 Detailed Classification Report ({best_model_name}):\")\\nprint(classification_report(y_test, best_predictions, target_names=[\\'Real\\', \\'Fake\\']))\\n\\n# Confusion Matrix\\ncm = confusion_matrix(y_test, best_predictions)\\nprint(f\"\\\\nüìä Confusion Matrix ({best_model_name}):\")\\nprint(\"Predicted:  Real  Fake\")\\nprint(f\"Real:       {cm[0][0]:4d}  {cm[0][1]:4d}\")\\nprint(f\"Fake:       {cm[1][0]:4d}  {cm[1][1]:4d}\")\\n\\n# ============================================================================\\n# 8. PREDICTION FUNCTION\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"STEP 7: PREDICTION FUNCTION\")\\nprint(\"=\" * 70)\\n\\ndef predict_fake_review(review_text, model_name=\\'Logistic Regression\\'):\\n    \"\"\"\\n    Predict if a review is fake using the specified model\\n    \"\"\"\\n    if model_name not in models:\\n        model_name = \\'Logistic Regression\\'\\n    \\n    # Preprocess text\\n    cleaned_text = preprocessor.preprocess(review_text)\\n    \\n    # Get model\\n    vectorizer, classifier = models[model_name]\\n    \\n    # Transform text\\n    text_vectorized = vectorizer.transform([cleaned_text])\\n    \\n    # Predict\\n    prediction = classifier.predict(text_vectorized)[0]\\n    probabilities = classifier.predict_proba(text_vectorized)[0]\\n    \\n    result = {\\n        \\'model\\': model_name,\\n        \\'prediction\\': \\'FAKE\\' if prediction == 1 else \\'REAL\\',\\n        \\'confidence\\': probabilities[prediction] * 100,\\n        \\'fake_probability\\': probabilities[1] * 100,\\n        \\'real_probability\\': probabilities[0] * 100\\n    }\\n    \\n    return result\\n\\n# Test the prediction function\\nprint(\"üß™ Testing Prediction Function:\")\\nprint(\"-\" * 50)\\n\\ntest_reviews = [\\n    \"This product is AMAZING!!!! Best purchase ever!!!!! Perfect perfect perfect!\",\\n    \"The product works as described. Good quality for the price.\",\\n    \"Worst product ever!!! Total waste of money!!!! Don\\'t buy this!\",\\n    \"Decent product, delivered on time. Fair quality for the price.\"\\n]\\n\\nfor i, review in enumerate(test_reviews, 1):\\n    print(f\"\\\\nTest Review {i}: {review[:60]}...\")\\n    \\n    for model_name in [\\'Logistic Regression\\', \\'Random Forest\\']:\\n        result = predict_fake_review(review, model_name)\\n        print(f\"  {model_name}: {result[\\'prediction\\']} ({result[\\'confidence\\']:.1f}% confidence)\")\\n\\n# ============================================================================\\n# 9. FEATURE IMPORTANCE ANALYSIS\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"STEP 8: FEATURE IMPORTANCE\")\\nprint(\"=\" * 70)\\n\\n# Random Forest feature importance\\nif \\'Random Forest\\' in models:\\n    _, rf_clf = models[\\'Random Forest\\']\\n    feature_names = tfidf.get_feature_names_out()\\n    importances = rf_clf.feature_importances_\\n    \\n    # Get top features\\n    feature_importance = list(zip(feature_names, importances))\\n    feature_importance.sort(key=lambda x: x[1], reverse=True)\\n    \\n    print(\"üîç Top 15 Most Important Features (Random Forest):\")\\n    print(\"-\" * 50)\\n    for i, (feature, importance) in enumerate(feature_importance[:15], 1):\\n        print(f\"{i:2d}. {feature:<20} {importance:.4f}\")\\n\\n# ============================================================================\\n# 10. SUMMARY\\n# ============================================================================\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"üéâ FAKE REVIEW DETECTION ANALYSIS COMPLETE!\")\\nprint(\"=\" * 70)\\n\\nprint(f\"\\\\nüìä Summary:\")\\nprint(\"-\" * 50)\\nprint(f\"   ‚úì Dataset size: {len(df)} reviews\")\\nprint(f\"   ‚úì Training samples: {len(X_text_train)}\")\\nprint(f\"   ‚úì Testing samples: {len(X_text_test)}\")\\nprint(f\"   ‚úì Models trained: {len(results)}\")\\nprint(f\"   ‚úì Best model: {best_model_name}\")\\nprint(f\"   ‚úì Best accuracy: {comparison_df.iloc[0][\\'Accuracy\\']:.4f}\")\\nprint(f\"   ‚úì Feature extraction: {text_features.shape[1]} linguistic features\")\\n\\nprint(f\"\\\\nüöÄ Usage:\")\\nprint(\"-\" * 50)\\nprint(\"Use predict_fake_review(\\'your review text\\') to classify new reviews\")\\nprint(\"Available models: Logistic Regression, Naive Bayes, Random Forest, Gradient Boosting\")\\n\\nprint(f\"\\\\nüí° Key Insights:\")\\nprint(\"-\" * 50)\\nprint(\"‚Ä¢ Fake reviews often contain excessive punctuation (!!!)\")\\nprint(\"‚Ä¢ Extreme words (amazing, terrible, perfect) are common in fake reviews\")\\nprint(\"‚Ä¢ Lexical diversity and sentence structure differ between real and fake reviews\")\\nprint(\"‚Ä¢ Traditional ML models can achieve good performance on this task\")\\n\\nprint(\"\\\\n\" + \"=\" * 70)\\nprint(\"Thank you for using the Fake Review Detector!\")\\nprint(\"=\" * 70)', 14241, 14242, 'surrogates not allowed')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'utf-8' codec can't encode character '\\udcc8' in position 10: surrogates not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3490\u001b[39m, in \u001b[36mInteractiveShell.transform_cell\u001b[39m\u001b[34m(self, raw_cell)\u001b[39m\n\u001b[32m   3477\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform an input cell before parsing it.\u001b[39;00m\n\u001b[32m   3478\u001b[39m \n\u001b[32m   3479\u001b[39m \u001b[33;03mStatic transformations, implemented in IPython.core.inputtransformer2,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3487\u001b[39m \u001b[33;03msee :meth:`transform_ast`.\u001b[39;00m\n\u001b[32m   3488\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3489\u001b[39m \u001b[38;5;66;03m# Static input transformations\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m cell = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_transformer_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cell.splitlines()) == \u001b[32m1\u001b[39m:\n\u001b[32m   3493\u001b[39m     \u001b[38;5;66;03m# Dynamic transformations - only applied for single line commands\u001b[39;00m\n\u001b[32m   3494\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   3495\u001b[39m         \u001b[38;5;66;03m# use prefilter_lines to handle trailing newlines\u001b[39;00m\n\u001b[32m   3496\u001b[39m         \u001b[38;5;66;03m# restore trailing newline for ast.parse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\inputtransformer2.py:643\u001b[39m, in \u001b[36mTransformerManager.transform_cell\u001b[39m\u001b[34m(self, cell)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cleanup_transforms + \u001b[38;5;28mself\u001b[39m.line_transforms:\n\u001b[32m    641\u001b[39m     lines = transform(lines)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_token_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(lines)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\inputtransformer2.py:628\u001b[39m, in \u001b[36mTransformerManager.do_token_transforms\u001b[39m\u001b[34m(self, lines)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_token_transforms\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines):\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRANSFORM_LOOP_LIMIT):\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m         changed, lines = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_one_token_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    629\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m changed:\n\u001b[32m    630\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m lines\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\inputtransformer2.py:608\u001b[39m, in \u001b[36mTransformerManager.do_one_token_transform\u001b[39m\u001b[34m(self, lines)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_one_token_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, lines):\n\u001b[32m    595\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Find and run the transform earliest in the code.\u001b[39;00m\n\u001b[32m    596\u001b[39m \n\u001b[32m    597\u001b[39m \u001b[33;03m    Returns (changed, lines).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    606\u001b[39m \u001b[33;03m    a performance issue.\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m     tokens_by_line = \u001b[43mmake_tokens_by_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m     candidates = []\n\u001b[32m    610\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transformer_cls \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_transformers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\inputtransformer2.py:532\u001b[39m, in \u001b[36mmake_tokens_by_line\u001b[39m\u001b[34m(lines)\u001b[39m\n\u001b[32m    530\u001b[39m parenlev = \u001b[32m0\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tokens_catch_errors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_errors_to_catch\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpected EOF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens_by_line\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEWLINE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mNL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mparenlev\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\utils\\tokenutil.py:45\u001b[39m, in \u001b[36mgenerate_tokens_catch_errors\u001b[39m\u001b[34m(readline, extra_errors_to_catch)\u001b[39m\n\u001b[32m     43\u001b[39m tokens: \u001b[38;5;28mlist\u001b[39m[TokenInfo] = []\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\tokenize.py:574\u001b[39m, in \u001b[36m_generate_tokens_from_c_tokenizer\u001b[39m\u001b[34m(source, encoding, extra_tokens)\u001b[39m\n\u001b[32m    572\u001b[39m     it = _tokenize.TokenizerIter(source, encoding=encoding, extra_tokens=extra_tokens)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTokenInfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'utf-8' codec can't encode character '\\udcc8' in position 10: surrogates not allowed"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Amazon Fake Review Detector - Traditional ML Pipeline\n",
    "=====================================================\n",
    "This notebook implements a fake review detection system using traditional ML techniques.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting Amazon Fake Review Detector...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             accuracy_score, precision_recall_fscore_support)\n",
    "\n",
    "# Try to import NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    # Download NLTK data\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"‚úÖ NLTK libraries imported and data downloaded!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è NLTK not available, using basic text processing\")\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "print(\"‚úÖ Core libraries imported successfully!\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. LOAD AND EXPLORE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: DATA EXPLORATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load local CSV files\n",
    "print(\"üìÅ Loading local CSV files...\")\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print(f\"‚úÖ Training data loaded: {df.shape}\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"Total training samples: {len(df)}\")\n",
    "    \n",
    "    if 'is_fake' in df.columns:\n",
    "        print(f\"Class Distribution:\\n{df['is_fake'].value_counts()}\")\n",
    "        print(f\"Fake percentage: {df['is_fake'].mean() * 100:.2f}%\")\n",
    "    \n",
    "    # Display sample reviews\n",
    "    print(f\"\\nSample Reviews:\")\n",
    "    for idx in range(min(3, len(df))):\n",
    "        label = 'FAKE' if df.iloc[idx]['is_fake'] == 1 else 'REAL' if 'is_fake' in df.columns else 'Unknown'\n",
    "        print(f\"{idx+1}. Label: {label}\")\n",
    "        if 'rating' in df.columns:\n",
    "            print(f\"   Rating: {df.iloc[idx]['rating']}\")\n",
    "        print(f\"   Text: {df.iloc[idx]['text'][:100]}...\")\n",
    "        print()\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå train.csv not found. Creating sample data for demonstration...\")\n",
    "    # Create sample data\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            \"This product is AMAZING!!!! Best purchase ever!!!!! So happy with it! Perfect perfect perfect!\",\n",
    "            \"The product works as described. Good quality for the price. Delivery was on time.\",\n",
    "            \"Terrible product! Complete waste of money! Don't buy this garbage! Worst ever!\",\n",
    "            \"Decent product, delivered on time. Average quality, nothing special.\",\n",
    "            \"BEST PRODUCT EVER!!! LOVE IT SO MUCH!!! BUY NOW!!! Amazing amazing amazing!\",\n",
    "            \"Good product, meets expectations. Fair price for what you get.\",\n",
    "            \"Horrible quality! Broke immediately! Total scam! Don't waste your money!\",\n",
    "            \"Nice product, well made. Would recommend to others. Good value.\",\n",
    "            \"FANTASTIC!!! INCREDIBLE!!! MUST BUY!!! Best thing ever made!!!\",\n",
    "            \"Average product. Does what it's supposed to do. Nothing more, nothing less.\"\n",
    "        ],\n",
    "        'rating': [5, 4, 1, 3, 5, 4, 1, 4, 5, 3],\n",
    "        'is_fake': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(f\"‚úÖ Sample data created with {len(df)} reviews\")\n",
    "    print(f\"Class Distribution:\\n{df['is_fake'].value_counts()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TEXT PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: TEXT PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        if NLTK_AVAILABLE:\n",
    "            try:\n",
    "                self.lemmatizer = WordNetLemmatizer()\n",
    "                self.stop_words = set(stopwords.words('english'))\n",
    "                print(\"‚úÖ NLTK preprocessing components loaded\")\n",
    "            except:\n",
    "                self.lemmatizer = None\n",
    "                self.stop_words = set()\n",
    "                print(\"‚ö†Ô∏è Using basic preprocessing (NLTK components failed)\")\n",
    "        else:\n",
    "            self.lemmatizer = None\n",
    "            # Basic stop words\n",
    "            self.stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
    "            print(\"‚úÖ Basic preprocessing components loaded\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove special characters but keep punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s!?.,]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "        if self.lemmatizer and NLTK_AVAILABLE:\n",
    "            try:\n",
    "                tokens = word_tokenize(text)\n",
    "                lemmatized = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "                return ' '.join(lemmatized)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Basic tokenization\n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        tokens = text.split()\n",
    "        filtered = [word for word in tokens if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered)\n",
    "    \n",
    "    def preprocess(self, text, remove_stops=True):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        text = self.tokenize_and_lemmatize(text)\n",
    "        if remove_stops:\n",
    "            text = self.remove_stopwords(text)\n",
    "        return text\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessor = TextPreprocessor()\n",
    "print(\"\\nüìù Preprocessing text data...\")\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: preprocessor.preprocess(x, remove_stops=True))\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete!\")\n",
    "print(\"\\nExample of preprocessed text:\")\n",
    "print(f\"Original:  {df.iloc[0]['text'][:80]}...\")\n",
    "print(f\"Cleaned:   {df.iloc[0]['cleaned_text'][:80]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def extract_text_features(df):\n",
    "    \"\"\"Extract linguistic features from text\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Basic text features\n",
    "    features['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    features['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "    features['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0)\n",
    "    \n",
    "    # Sentence features\n",
    "    features['sentence_count'] = df['text'].apply(lambda x: len(re.split(r'[.!?]+', str(x))))\n",
    "    features['avg_sentence_length'] = features['word_count'] / features['sentence_count'].replace(0, 1)\n",
    "    \n",
    "    # Punctuation features\n",
    "    features['exclamation_count'] = df['text'].apply(lambda x: str(x).count('!'))\n",
    "    features['question_count'] = df['text'].apply(lambda x: str(x).count('?'))\n",
    "    features['exclamation_ratio'] = features['exclamation_count'] / (features['word_count'] + 1)\n",
    "    \n",
    "    # Capital letters\n",
    "    features['capital_ratio'] = df['text'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / len(str(x)) if len(str(x)) > 0 else 0)\n",
    "    \n",
    "    # Lexical diversity\n",
    "    features['unique_word_ratio'] = df['text'].apply(lambda x: len(set(str(x).split())) / len(str(x).split()) if len(str(x).split()) > 0 else 0)\n",
    "    \n",
    "    # Extreme words (common in fake reviews)\n",
    "    extreme_words = ['amazing', 'awful', 'terrible', 'perfect', 'worst', 'best', 'horrible', 'excellent', 'fantastic', 'incredible']\n",
    "    features['extreme_word_count'] = df['text'].apply(lambda x: sum(str(x).lower().count(word) for word in extreme_words))\n",
    "    features['extreme_word_ratio'] = features['extreme_word_count'] / (features['word_count'] + 1)\n",
    "    \n",
    "    # Rating features (if available)\n",
    "    if 'rating' in df.columns:\n",
    "        features['rating'] = df['rating']\n",
    "        features['is_extreme_rating'] = df['rating'].apply(lambda x: 1 if x in [1, 2, 5] else 0)\n",
    "    else:\n",
    "        features['rating'] = 3  # neutral default\n",
    "        features['is_extreme_rating'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"üîß Extracting linguistic features...\")\n",
    "text_features = extract_text_features(df)\n",
    "print(f\"‚úÖ Extracted {text_features.shape[1]} features\")\n",
    "print(\"\\nFeature Summary:\")\n",
    "print(text_features.describe())\n",
    "\n",
    "# ============================================================================\n",
    "# 5. PREPARE DATA FOR MODELING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: PREPARE DATA FOR MODELING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare data\n",
    "X_text = df['cleaned_text']\n",
    "X_features = text_features\n",
    "y = df['is_fake']\n",
    "\n",
    "# Train-test split\n",
    "X_text_train, X_text_test, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_features_train, X_features_test = train_test_split(\n",
    "    X_features, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data split complete!\")\n",
    "print(f\"Training samples: {len(X_text_train)}\")\n",
    "print(f\"Testing samples: {len(X_text_test)}\")\n",
    "print(f\"Training class balance: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. TRADITIONAL ML MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: TRADITIONAL ML MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), min_df=1, max_df=0.95)\n",
    "X_train_tfidf = tfidf.fit_transform(X_text_train)\n",
    "X_test_tfidf = tfidf.transform(X_text_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Model 1: Logistic Regression\n",
    "print(\"\\nü§ñ Training Model 1: Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_pred = lr_model.predict(X_test_tfidf)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "models['Logistic Regression'] = (tfidf, lr_model)\n",
    "results['Logistic Regression'] = {'accuracy': lr_acc, 'predictions': lr_pred}\n",
    "print(f\"‚úÖ Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
    "\n",
    "# Model 2: Naive Bayes\n",
    "print(\"\\nü§ñ Training Model 2: Naive Bayes...\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_pred = nb_model.predict(X_test_tfidf)\n",
    "nb_acc = accuracy_score(y_test, nb_pred)\n",
    "\n",
    "models['Naive Bayes'] = (tfidf, nb_model)\n",
    "results['Naive Bayes'] = {'accuracy': nb_acc, 'predictions': nb_pred}\n",
    "print(f\"‚úÖ Naive Bayes Accuracy: {nb_acc:.4f}\")\n",
    "\n",
    "# Model 3: Random Forest\n",
    "print(\"\\nü§ñ Training Model 3: Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "rf_pred = rf_model.predict(X_test_tfidf)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "models['Random Forest'] = (tfidf, rf_model)\n",
    "results['Random Forest'] = {'accuracy': rf_acc, 'predictions': rf_pred}\n",
    "print(f\"‚úÖ Random Forest Accuracy: {rf_acc:.4f}\")\n",
    "\n",
    "# Model 4: Gradient Boosting\n",
    "print(\"\\nü§ñ Training Model 4: Gradient Boosting...\")\n",
    "gb_model = GradientBoostingClassifier(n_estimators=50, random_state=42, max_depth=5)\n",
    "gb_model.fit(X_train_tfidf, y_train)\n",
    "gb_pred = gb_model.predict(X_test_tfidf)\n",
    "gb_acc = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "models['Gradient Boosting'] = (tfidf, gb_model)\n",
    "results['Gradient Boosting'] = {'accuracy': gb_acc, 'predictions': gb_pred}\n",
    "print(f\"‚úÖ Gradient Boosting Accuracy: {gb_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Model comparison\n",
    "comparison_data = []\n",
    "for model_name in results.keys():\n",
    "    preds = results[model_name]['predictions']\n",
    "    acc = results[model_name]['accuracy']\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary', zero_division=0)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"üìä Model Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<12} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['Model']:<20} {row['Accuracy']:<10.4f} {row['Precision']:<12.4f} {row['Recall']:<10.4f} {row['F1-Score']:<10.4f}\")\n",
    "\n",
    "# Best model analysis\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"Best Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n\udcc8 Detailed Classification Report ({best_model_name}):\")\n",
    "print(classification_report(y_test, best_predictions, target_names=['Real', 'Fake']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "print(f\"\\nüìä Confusion Matrix ({best_model_name}):\")\n",
    "print(\"Predicted:  Real  Fake\")\n",
    "print(f\"Real:       {cm[0][0]:4d}  {cm[0][1]:4d}\")\n",
    "print(f\"Fake:       {cm[1][0]:4d}  {cm[1][1]:4d}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. PREDICTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 7: PREDICTION FUNCTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def predict_fake_review(review_text, model_name='Logistic Regression'):\n",
    "    \"\"\"\n",
    "    Predict if a review is fake using the specified model\n",
    "    \"\"\"\n",
    "    if model_name not in models:\n",
    "        model_name = 'Logistic Regression'\n",
    "    \n",
    "    # Preprocess text\n",
    "    cleaned_text = preprocessor.preprocess(review_text)\n",
    "    \n",
    "    # Get model\n",
    "    vectorizer, classifier = models[model_name]\n",
    "    \n",
    "    # Transform text\n",
    "    text_vectorized = vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = classifier.predict(text_vectorized)[0]\n",
    "    probabilities = classifier.predict_proba(text_vectorized)[0]\n",
    "    \n",
    "    result = {\n",
    "        'model': model_name,\n",
    "        'prediction': 'FAKE' if prediction == 1 else 'REAL',\n",
    "        'confidence': probabilities[prediction] * 100,\n",
    "        'fake_probability': probabilities[1] * 100,\n",
    "        'real_probability': probabilities[0] * 100\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the prediction function\n",
    "print(\"üß™ Testing Prediction Function:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "test_reviews = [\n",
    "    \"This product is AMAZING!!!! Best purchase ever!!!!! Perfect perfect perfect!\",\n",
    "    \"The product works as described. Good quality for the price.\",\n",
    "    \"Worst product ever!!! Total waste of money!!!! Don't buy this!\",\n",
    "    \"Decent product, delivered on time. Fair quality for the price.\"\n",
    "]\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    print(f\"\\nTest Review {i}: {review[:60]}...\")\n",
    "    \n",
    "    for model_name in ['Logistic Regression', 'Random Forest']:\n",
    "        result = predict_fake_review(review, model_name)\n",
    "        print(f\"  {model_name}: {result['prediction']} ({result['confidence']:.1f}% confidence)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 8: FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Random Forest feature importance\n",
    "if 'Random Forest' in models:\n",
    "    _, rf_clf = models['Random Forest']\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    importances = rf_clf.feature_importances_\n",
    "    \n",
    "    # Get top features\n",
    "    feature_importance = list(zip(feature_names, importances))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"üîç Top 15 Most Important Features (Random Forest):\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (feature, importance) in enumerate(feature_importance[:15], 1):\n",
    "        print(f\"{i:2d}. {feature:<20} {importance:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ FAKE REVIEW DETECTION ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   ‚úì Dataset size: {len(df)} reviews\")\n",
    "print(f\"   ‚úì Training samples: {len(X_text_train)}\")\n",
    "print(f\"   ‚úì Testing samples: {len(X_text_test)}\")\n",
    "print(f\"   ‚úì Models trained: {len(results)}\")\n",
    "print(f\"   ‚úì Best model: {best_model_name}\")\n",
    "print(f\"   ‚úì Best accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
    "print(f\"   ‚úì Feature extraction: {text_features.shape[1]} linguistic features\")\n",
    "\n",
    "print(f\"\\nüöÄ Usage:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Use predict_fake_review('your review text') to classify new reviews\")\n",
    "print(\"Available models: Logistic Regression, Naive Bayes, Random Forest, Gradient Boosting\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"‚Ä¢ Fake reviews often contain excessive punctuation (!!!)\")\n",
    "print(\"‚Ä¢ Extreme words (amazing, terrible, perfect) are common in fake reviews\")\n",
    "print(\"‚Ä¢ Lexical diversity and sentence structure differ between real and fake reviews\")\n",
    "print(\"‚Ä¢ Traditional ML models can achieve good performance on this task\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Thank you for using the Fake Review Detector!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09fb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b501903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Amazon Fake Review Detector ===\n",
      "‚úÖ Libraries imported successfully!\n",
      "‚úÖ Data loaded: (3599999, 3)\n",
      "Warning: 'is_fake' column not found\n",
      "üìù Preprocessing text...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù Preprocessing text...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(clean_text)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[32m     55\u001b[39m X = df[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\91983\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'text'"
     ]
    }
   ],
   "source": [
    "# Amazon Fake Review Detector - Working Version\n",
    "print(\"=== Amazon Fake Review Detector ===\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print(f\"‚úÖ Data loaded: {df.shape}\")\n",
    "    if 'is_fake' in df.columns:\n",
    "        print(f\"Class distribution: {df['is_fake'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"Warning: 'is_fake' column not found\")\n",
    "except Exception as e:\n",
    "    print(f\"Creating sample data (Error: {e})\")\n",
    "    df = pd.DataFrame({\n",
    "        'text': [\n",
    "            \"This product is AMAZING!!! Best ever!!!\",\n",
    "            \"Good quality product, works as expected.\",\n",
    "            \"Terrible! Complete waste of money!\",\n",
    "            \"Decent product for the price.\",\n",
    "            \"PERFECT!!! LOVE IT!!! BUY NOW!!!\",\n",
    "            \"Fair quality, nothing special.\",\n",
    "            \"Horrible quality! Don't buy!\",\n",
    "            \"Nice product, good value.\",\n",
    "            \"INCREDIBLE!!! MUST HAVE!!!\",\n",
    "            \"Average product, does the job.\"\n",
    "        ] * 5,  # Repeat for more data\n",
    "        'rating': [5, 4, 1, 3, 5, 3, 1, 4, 5, 3] * 5,\n",
    "        'is_fake': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0] * 5\n",
    "    })\n",
    "    print(f\"‚úÖ Sample data created: {df.shape}\")\n",
    "\n",
    "# Simple text preprocessing\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"üìù Preprocessing text...\")\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Split data\n",
    "X = df['cleaned_text']\n",
    "y = df['is_fake']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"üìä Training samples: {len(X_train)}\")\n",
    "print(f\"üìä Test samples: {len(X_test)}\")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"üî¢ Feature matrix shape: {X_train_vec.shape}\")\n",
    "\n",
    "# Train models\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "print(\"\\nü§ñ Training Models...\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train_vec, y_train)\n",
    "lr_pred = lr.predict(X_test_vec)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "models['Logistic Regression'] = lr\n",
    "results['Logistic Regression'] = lr_acc\n",
    "print(f\"  Logistic Regression: {lr_acc:.4f}\")\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "nb_pred = nb.predict(X_test_vec)\n",
    "nb_acc = accuracy_score(y_test, nb_pred)\n",
    "models['Naive Bayes'] = nb\n",
    "results['Naive Bayes'] = nb_acc\n",
    "print(f\"  Naive Bayes: {nb_acc:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X_train_vec, y_train)\n",
    "rf_pred = rf.predict(X_test_vec)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "models['Random Forest'] = rf\n",
    "results['Random Forest'] = rf_acc\n",
    "print(f\"  Random Forest: {rf_acc:.4f}\")\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "for model, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {model}: {acc:.4f}\")\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"\\nüèÜ Best Model: {best_model} ({results[best_model]:.4f})\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_review(text, model_name=None):\n",
    "    if model_name is None:\n",
    "        model_name = best_model\n",
    "    \n",
    "    cleaned = clean_text(text)\n",
    "    vectorized = vectorizer.transform([cleaned])\n",
    "    prediction = models[model_name].predict(vectorized)[0]\n",
    "    probabilities = models[model_name].predict_proba(vectorized)[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'FAKE' if prediction == 1 else 'REAL',\n",
    "        'confidence': max(probabilities) * 100,\n",
    "        'fake_prob': probabilities[1] * 100 if len(probabilities) > 1 else 0,\n",
    "        'real_prob': probabilities[0] * 100 if len(probabilities) > 1 else 100\n",
    "    }\n",
    "\n",
    "# Test predictions\n",
    "test_texts = [\n",
    "    \"This product is AMAZING!!! Best purchase ever!!! PERFECT!!!\",\n",
    "    \"Good quality product, works as described. Fair price.\",\n",
    "    \"Terrible product! Complete waste of money! Don't buy!\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Test Predictions:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = predict_review(text)\n",
    "    print(f\"  {i}. Text: '{text[:40]}...'\")\n",
    "    print(f\"     Prediction: {result['prediction']} (Confidence: {result['confidence']:.1f}%)\")\n",
    "    print(f\"     Fake: {result['fake_prob']:.1f}% | Real: {result['real_prob']:.1f}%\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(\"üöÄ Use predict_review('your text') to classify new reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a092b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data structure...\n",
      "Data shape: (3599999, 3)\n",
      "Columns: ['2', 'Stuning even for the non-gamer', 'This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^']\n",
      "First few rows:\n",
      "   2                     Stuning even for the non-gamer  \\\n",
      "0  2              The best soundtrack ever to anything.   \n",
      "1  2                                           Amazing!   \n",
      "2  2                               Excellent Soundtrack   \n",
      "3  2  Remember, Pull Your Jaw Off The Floor After He...   \n",
      "4  2                            an absolute masterpiece   \n",
      "\n",
      "  This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^  \n",
      "0  I'm reading a lot of reviews saying that this ...                                                                                                                                                                                                                                                                                                                                                          \n",
      "1  This soundtrack is my favorite music of all ti...                                                                                                                                                                                                                                                                                                                                                          \n",
      "2  I truly like this soundtrack and I enjoy video...                                                                                                                                                                                                                                                                                                                                                          \n",
      "3  If you've played the game, you know how divine...                                                                                                                                                                                                                                                                                                                                                          \n",
      "4  I am quite sure any of you actually taking the...                                                                                                                                                                                                                                                                                                                                                          \n",
      "\n",
      "Data types:\n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                              int64\n",
      "Stuning even for the non-gamer                                                                                                                                                                                                                                                                                                                                                                                object\n",
      "This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^    object\n",
      "dtype: object\n",
      "\n",
      "Test data shape: (399999, 3)\n",
      "Test columns: ['2', 'Great CD', 'My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I\\'m in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life\\'s hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"']\n"
     ]
    }
   ],
   "source": [
    "# Check the data structure first\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Checking data structure...\")\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"First few rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    \n",
    "# Also check test.csv\n",
    "try:\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(f\"\\nTest data shape: {test_df.shape}\")\n",
    "    print(f\"Test columns: {list(test_df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ef26b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "0: '2'\n",
      "1: 'Stuning even for the non-gamer'\n",
      "2: 'This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^'\n",
      "\n",
      "Shape: (3599999, 3)\n",
      "Sample values from first row:\n",
      "2: 2\n",
      "Stuning even for the non-gamer: The best soundtrack ever to anything.\n",
      "This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\n"
     ]
    }
   ],
   "source": [
    "# Quick check of column names\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "print(\"Column names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i}: '{col}'\")\n",
    "\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(\"Sample values from first row:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].iloc[0] if len(df) > 0 else 'No data'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ea12bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Amazon Fake Review Detector ===\n",
      "‚úÖ Libraries imported successfully!\n",
      "‚úÖ Data loaded and processed: (3600000, 4)\n",
      "Columns: ['rating', 'title', 'text', 'is_fake']\n",
      "Class distribution: {0: 3259610, 1: 340390}\n",
      "Using sample of 10000 reviews for training\n",
      "üìù Preprocessing text...\n",
      "üîß Extracting features...\n",
      "üìä Training samples: 7000\n",
      "üìä Test samples: 3000\n",
      "üî¢ TF-IDF matrix shape: (7000, 500)\n",
      "\n",
      "ü§ñ Training Models...\n",
      "  ‚úÖ Logistic Regression: 0.9130\n",
      "  ‚úÖ Naive Bayes: 0.9117\n",
      "  ‚úÖ Random Forest: 0.9117\n",
      "\n",
      "üìä Model Performance:\n",
      "----------------------------------------\n",
      "  Logistic Regression : 0.9130\n",
      "  Naive Bayes         : 0.9117\n",
      "  Random Forest       : 0.9117\n",
      "\n",
      "üèÜ Best Model: Logistic Regression (0.9130)\n",
      "\n",
      "üìà Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.91      1.00      0.95      2735\n",
      "        Fake       0.67      0.03      0.06       265\n",
      "\n",
      "    accuracy                           0.91      3000\n",
      "   macro avg       0.79      0.51      0.51      3000\n",
      "weighted avg       0.89      0.91      0.88      3000\n",
      "\n",
      "\n",
      "üß™ Test Predictions:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Review: 'This product is AMAZING!!! Best purchase ever!!! P...'\n",
      "   Prediction: FAKE (Confidence: 80.7%)\n",
      "   Probabilities: Fake 80.7% | Real 19.3%\n",
      "\n",
      "2. Review: 'Good quality product. Works as described. Fair pri...'\n",
      "   Prediction: REAL (Confidence: 94.4%)\n",
      "   Probabilities: Fake 5.6% | Real 94.4%\n",
      "\n",
      "3. Review: 'Terrible product! Complete waste of money! Worst t...'\n",
      "   Prediction: REAL (Confidence: 82.8%)\n",
      "   Probabilities: Fake 17.2% | Real 82.8%\n",
      "\n",
      "4. Review: 'Decent product. Nothing special but does the job. ...'\n",
      "   Prediction: REAL (Confidence: 96.1%)\n",
      "   Probabilities: Fake 3.9% | Real 96.1%\n",
      "\n",
      "üîç Top Features (Random Forest):\n",
      "   1. best           : 0.0265\n",
      "   2. the best       : 0.0179\n",
      "   3. perfect        : 0.0115\n",
      "   4. the            : 0.0099\n",
      "   5. and            : 0.0099\n",
      "   6. this           : 0.0090\n",
      "   7. to             : 0.0080\n",
      "   8. all            : 0.0077\n",
      "   9. is             : 0.0076\n",
      "  10. of             : 0.0076\n",
      "\n",
      "‚úÖ Analysis Complete!\n",
      "üöÄ Use predict_fake_review('your review text') to classify new reviews\n",
      "üìä Dataset: 10000 reviews processed\n",
      "üéØ Best accuracy: 0.9130 with Logistic Regression\n",
      "\n",
      "============================================================\n",
      "Fake Review Detection Model Ready!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Amazon Fake Review Detector - Final Working Version\n",
    "print(\"=== Amazon Fake Review Detector ===\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "\n",
    "# Load data with proper headers\n",
    "try:\n",
    "    # Load without headers first\n",
    "    df = pd.read_csv('train.csv', header=None)\n",
    "    # Assign proper column names\n",
    "    df.columns = ['rating', 'title', 'text']\n",
    "    \n",
    "    # For this example, let's create a simple fake detection based on text patterns\n",
    "    # In real data, you would have actual labels\n",
    "    def detect_fake_patterns(text):\n",
    "        text = str(text).lower()\n",
    "        # Simple heuristic: excessive punctuation and extreme words\n",
    "        exclamation_count = text.count('!')\n",
    "        extreme_words = ['amazing', 'terrible', 'perfect', 'worst', 'best', 'incredible', 'awful']\n",
    "        extreme_count = sum(text.count(word) for word in extreme_words)\n",
    "        \n",
    "        # If lots of exclamations or extreme words, likely fake\n",
    "        if exclamation_count >= 3 or extreme_count >= 2:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    df['is_fake'] = df['text'].apply(detect_fake_patterns)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded and processed: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Class distribution: {df['is_fake'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Sample the data for faster processing\n",
    "    df_sample = df.sample(n=min(10000, len(df)), random_state=42)\n",
    "    print(f\"Using sample of {len(df_sample)} reviews for training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with real data: {e}\")\n",
    "    print(\"Using sample data instead...\")\n",
    "    df_sample = pd.DataFrame({\n",
    "        'text': [\n",
    "            \"This product is AMAZING!!! Best ever!!! Perfect!!!\",\n",
    "            \"Good quality product, works as expected.\",\n",
    "            \"Terrible! Complete waste of money! Awful!!!\",\n",
    "            \"Decent product for the price. Fair quality.\",\n",
    "            \"PERFECT!!! LOVE IT!!! BUY NOW!!! Incredible!!!\",\n",
    "            \"Fair quality, nothing special to mention.\",\n",
    "            \"Horrible quality! Don't buy! Worst ever!!!\",\n",
    "            \"Nice product, good value for money.\",\n",
    "            \"INCREDIBLE!!! MUST HAVE!!! Amazing quality!!!\",\n",
    "            \"Average product, does what it should do.\"\n",
    "        ] * 20,  # Repeat for more data\n",
    "        'rating': [5, 4, 1, 3, 5, 3, 1, 4, 5, 3] * 20,\n",
    "        'is_fake': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0] * 20\n",
    "    })\n",
    "\n",
    "# Text preprocessing\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    # Remove special characters but keep some punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s!?.]', '', text)\n",
    "    # Normalize multiple exclamations\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"üìù Preprocessing text...\")\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(clean_text)\n",
    "\n",
    "# Feature extraction\n",
    "def extract_features(text):\n",
    "    # Count features that might indicate fake reviews\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    uppercase_ratio = sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "    # Extreme words\n",
    "    extreme_words = ['amazing', 'terrible', 'perfect', 'worst', 'best', 'incredible', 'awful', 'fantastic']\n",
    "    extreme_count = sum(text.lower().count(word) for word in extreme_words)\n",
    "    \n",
    "    return [exclamation_count, question_count, word_count, char_count, uppercase_ratio, extreme_count]\n",
    "\n",
    "print(\"üîß Extracting features...\")\n",
    "feature_matrix = df_sample['text'].apply(extract_features).tolist()\n",
    "feature_df = pd.DataFrame(feature_matrix, columns=['exclamations', 'questions', 'word_count', 'char_count', 'uppercase_ratio', 'extreme_words'])\n",
    "\n",
    "# Split data\n",
    "X_text = df_sample['cleaned_text']\n",
    "X_features = feature_df\n",
    "y = df_sample['is_fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"üìä Training samples: {len(X_train)}\")\n",
    "print(f\"üìä Test samples: {len(X_test)}\")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2), min_df=2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"üî¢ TF-IDF matrix shape: {X_train_vec.shape}\")\n",
    "\n",
    "# Train models\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "print(\"\\nü§ñ Training Models...\")\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_train_vec, y_train)\n",
    "lr_pred = lr.predict(X_test_vec)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "models['Logistic Regression'] = lr\n",
    "results['Logistic Regression'] = lr_acc\n",
    "print(f\"  ‚úÖ Logistic Regression: {lr_acc:.4f}\")\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "nb_pred = nb.predict(X_test_vec)\n",
    "nb_acc = accuracy_score(y_test, nb_pred)\n",
    "models['Naive Bayes'] = nb\n",
    "results['Naive Bayes'] = nb_acc\n",
    "print(f\"  ‚úÖ Naive Bayes: {nb_acc:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X_train_vec, y_train)\n",
    "rf_pred = rf.predict(X_test_vec)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "models['Random Forest'] = rf\n",
    "results['Random Forest'] = rf_acc\n",
    "print(f\"  ‚úÖ Random Forest: {rf_acc:.4f}\")\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\nüìä Model Performance:\")\n",
    "print(\"-\" * 40)\n",
    "for model, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {model:<20}: {acc:.4f}\")\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"\\nüèÜ Best Model: {best_model} ({results[best_model]:.4f})\")\n",
    "\n",
    "# Detailed evaluation for best model\n",
    "best_pred = lr_pred if best_model == 'Logistic Regression' else (nb_pred if best_model == 'Naive Bayes' else rf_pred)\n",
    "print(f\"\\nüìà Classification Report ({best_model}):\")\n",
    "print(classification_report(y_test, best_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "# Prediction function\n",
    "def predict_fake_review(text, model_name=None):\n",
    "    if model_name is None:\n",
    "        model_name = best_model\n",
    "    \n",
    "    cleaned = clean_text(text)\n",
    "    vectorized = vectorizer.transform([cleaned])\n",
    "    prediction = models[model_name].predict(vectorized)[0]\n",
    "    probabilities = models[model_name].predict_proba(vectorized)[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'FAKE' if prediction == 1 else 'REAL',\n",
    "        'confidence': max(probabilities) * 100,\n",
    "        'fake_probability': probabilities[1] * 100,\n",
    "        'real_probability': probabilities[0] * 100\n",
    "    }\n",
    "\n",
    "# Test the model\n",
    "test_reviews = [\n",
    "    \"This product is AMAZING!!! Best purchase ever!!! Perfect quality!!! Buy now!!!\",\n",
    "    \"Good quality product. Works as described. Fair price for what you get.\",\n",
    "    \"Terrible product! Complete waste of money! Worst thing ever! Don't buy!\",\n",
    "    \"Decent product. Nothing special but does the job. Average quality.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Test Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "for i, text in enumerate(test_reviews, 1):\n",
    "    result = predict_fake_review(text)\n",
    "    print(f\"\\n{i}. Review: '{text[:50]}...'\")\n",
    "    print(f\"   Prediction: {result['prediction']} (Confidence: {result['confidence']:.1f}%)\")\n",
    "    print(f\"   Probabilities: Fake {result['fake_probability']:.1f}% | Real {result['real_probability']:.1f}%\")\n",
    "\n",
    "# Feature importance\n",
    "if 'Random Forest' in models:\n",
    "    print(f\"\\nüîç Top Features (Random Forest):\")\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    importances = models['Random Forest'].feature_importances_\n",
    "    top_features = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    for i, (feature, importance) in enumerate(top_features, 1):\n",
    "        print(f\"  {i:2d}. {feature:<15}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis Complete!\")\n",
    "print(f\"üöÄ Use predict_fake_review('your review text') to classify new reviews\")\n",
    "print(f\"üìä Dataset: {len(df_sample)} reviews processed\")\n",
    "print(f\"üéØ Best accuracy: {results[best_model]:.4f} with {best_model}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Fake Review Detection Model Ready!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e99178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving the Fake Review Detection Model...\n",
      "==================================================\n",
      "üìÅ Created directory: saved_models\n",
      "‚úÖ Best model (Logistic Regression) saved to: saved_models\\fake_review_detector_20251031_224832_best_model.pkl\n",
      "‚úÖ TF-IDF vectorizer saved to: saved_models\\fake_review_detector_20251031_224832_vectorizer.pkl\n",
      "‚úÖ All models saved to: saved_models\\fake_review_detector_20251031_224832_all_models.pkl\n",
      "‚úÖ Complete model package saved to: saved_models\\fake_review_detector_20251031_224832_complete_package.pkl\n",
      "‚úÖ Standalone prediction script saved to: saved_models\\fake_review_detector_20251031_224832_predictor.py\n",
      "‚úÖ Performance report saved to: saved_models\\fake_review_detector_20251031_224832_report.txt\n",
      "\n",
      "üß™ Testing saved model...\n",
      "‚úÖ Model loading test successful!\n",
      "   Test text: 'This product is AMAZING!!! Best purchase ever!!!'\n",
      "   Prediction: REAL\n",
      "   Confidence: 50.2%\n",
      "\n",
      "============================================================\n",
      "üéâ MODEL SUCCESSFULLY SAVED!\n",
      "============================================================\n",
      "üìÅ All files saved in: saved_models/\n",
      "üöÄ Use fake_review_detector_20251031_224832_predictor.py for standalone predictions\n",
      "üìä Model accuracy: 0.9130\n",
      "ü§ñ Best model: Logistic Regression\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model and components\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"üíæ Saving the Fake Review Detection Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a models directory if it doesn't exist\n",
    "models_dir = \"saved_models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"üìÅ Created directory: {models_dir}\")\n",
    "\n",
    "# Get current timestamp for unique filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_filename = f\"fake_review_detector_{timestamp}\"\n",
    "\n",
    "# 1. Save the best model (Logistic Regression)\n",
    "best_model_path = os.path.join(models_dir, f\"{base_filename}_best_model.pkl\")\n",
    "joblib.dump(models[best_model], best_model_path)\n",
    "print(f\"‚úÖ Best model ({best_model}) saved to: {best_model_path}\")\n",
    "\n",
    "# 2. Save the TF-IDF vectorizer\n",
    "vectorizer_path = os.path.join(models_dir, f\"{base_filename}_vectorizer.pkl\")\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "print(f\"‚úÖ TF-IDF vectorizer saved to: {vectorizer_path}\")\n",
    "\n",
    "# 3. Save all models dictionary\n",
    "all_models_path = os.path.join(models_dir, f\"{base_filename}_all_models.pkl\")\n",
    "joblib.dump(models, all_models_path)\n",
    "print(f\"‚úÖ All models saved to: {all_models_path}\")\n",
    "\n",
    "# 4. Save preprocessing functions and model metadata\n",
    "model_components = {\n",
    "    'best_model_name': best_model,\n",
    "    'model_accuracy': results[best_model],\n",
    "    'vectorizer': vectorizer,\n",
    "    'models': models,\n",
    "    'results': results,\n",
    "    'clean_text_function': clean_text,\n",
    "    'extract_features_function': extract_features,\n",
    "    'feature_names': vectorizer.get_feature_names_out(),\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'data_shape': df_sample.shape,\n",
    "    'class_distribution': y.value_counts().to_dict()\n",
    "}\n",
    "\n",
    "components_path = os.path.join(models_dir, f\"{base_filename}_complete_package.pkl\")\n",
    "with open(components_path, 'wb') as f:\n",
    "    pickle.dump(model_components, f)\n",
    "print(f\"‚úÖ Complete model package saved to: {components_path}\")\n",
    "\n",
    "# 5. Create a standalone prediction function and save it\n",
    "prediction_code = f'''\n",
    "import pickle\n",
    "import joblib\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model components\n",
    "def load_model(model_path=\"{components_path}\"):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        components = pickle.load(f)\n",
    "    return components\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\\\s!?.]', '', text)\n",
    "    text = re.sub(r'!+', '!', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def predict_fake_review(text, model_components=None):\n",
    "    if model_components is None:\n",
    "        model_components = load_model()\n",
    "    \n",
    "    # Preprocess text\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    vectorized = model_components['vectorizer'].transform([cleaned])\n",
    "    \n",
    "    # Predict using best model\n",
    "    best_model = model_components['models'][model_components['best_model_name']]\n",
    "    prediction = best_model.predict(vectorized)[0]\n",
    "    probabilities = best_model.predict_proba(vectorized)[0]\n",
    "    \n",
    "    return {{\n",
    "        'prediction': 'FAKE' if prediction == 1 else 'REAL',\n",
    "        'confidence': max(probabilities) * 100,\n",
    "        'fake_probability': probabilities[1] * 100,\n",
    "        'real_probability': probabilities[0] * 100,\n",
    "        'model_used': model_components['best_model_name']\n",
    "    }}\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the loaded model\n",
    "    test_text = \"This product is AMAZING!!! Best purchase ever!!!\"\n",
    "    result = predict_fake_review(test_text)\n",
    "    print(f\"Text: {{test_text}}\")\n",
    "    print(f\"Prediction: {{result['prediction']}} ({{result['confidence']:.1f}}% confidence)\")\n",
    "'''\n",
    "\n",
    "# Save the standalone prediction script\n",
    "script_path = os.path.join(models_dir, f\"{base_filename}_predictor.py\")\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(prediction_code)\n",
    "print(f\"‚úÖ Standalone prediction script saved to: {script_path}\")\n",
    "\n",
    "# 6. Save model performance report\n",
    "report_content = f\"\"\"\n",
    "Fake Review Detection Model Performance Report\n",
    "============================================\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Dataset Information:\n",
    "- Total reviews processed: {len(df_sample):,}\n",
    "- Training samples: {len(X_train):,}\n",
    "- Test samples: {len(X_test):,}\n",
    "- Class distribution: {y.value_counts().to_dict()}\n",
    "\n",
    "Model Performance:\n",
    "\"\"\"\n",
    "\n",
    "for model_name, accuracy in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    report_content += f\"- {model_name}: {accuracy:.4f}\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "Best Model: {best_model} (Accuracy: {results[best_model]:.4f})\n",
    "\n",
    "Feature Engineering:\n",
    "- TF-IDF Vectorization with bi-grams\n",
    "- {len(vectorizer.get_feature_names_out())} features extracted\n",
    "- Text preprocessing with pattern detection\n",
    "\n",
    "Saved Files:\n",
    "- Best model: {best_model_path}\n",
    "- Vectorizer: {vectorizer_path}\n",
    "- All models: {all_models_path}\n",
    "- Complete package: {components_path}\n",
    "- Prediction script: {script_path}\n",
    "\n",
    "Usage:\n",
    "To use the saved model, run:\n",
    "    python {base_filename}_predictor.py\n",
    "\n",
    "Or load in Python:\n",
    "    import pickle\n",
    "    with open('{components_path}', 'rb') as f:\n",
    "        model_components = pickle.load(f)\n",
    "\"\"\"\n",
    "\n",
    "report_path = os.path.join(models_dir, f\"{base_filename}_report.txt\")\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "print(f\"‚úÖ Performance report saved to: {report_path}\")\n",
    "\n",
    "# Test loading the saved model to ensure it works\n",
    "print(\"\\nüß™ Testing saved model...\")\n",
    "try:\n",
    "    # Load and test\n",
    "    with open(components_path, 'rb') as f:\n",
    "        loaded_components = pickle.load(f)\n",
    "    \n",
    "    # Test prediction\n",
    "    test_text = \"This product is AMAZING!!! Best purchase ever!!!\"\n",
    "    loaded_vectorizer = loaded_components['vectorizer']\n",
    "    loaded_best_model = loaded_components['models'][loaded_components['best_model_name']]\n",
    "    \n",
    "    cleaned = clean_text(test_text)\n",
    "    vectorized = loaded_vectorizer.transform([cleaned])\n",
    "    prediction = loaded_best_model.predict(vectorized)[0]\n",
    "    probabilities = loaded_best_model.predict_proba(vectorized)[0]\n",
    "    \n",
    "    print(f\"‚úÖ Model loading test successful!\")\n",
    "    print(f\"   Test text: '{test_text}'\")\n",
    "    print(f\"   Prediction: {'FAKE' if prediction == 1 else 'REAL'}\")\n",
    "    print(f\"   Confidence: {max(probabilities) * 100:.1f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing saved model: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ MODEL SUCCESSFULLY SAVED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ All files saved in: {models_dir}/\")\n",
    "print(f\"üöÄ Use {base_filename}_predictor.py for standalone predictions\")\n",
    "print(f\"üìä Model accuracy: {results[best_model]:.4f}\")\n",
    "print(f\"ü§ñ Best model: {best_model}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1340369,
     "sourceId": 2233682,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
